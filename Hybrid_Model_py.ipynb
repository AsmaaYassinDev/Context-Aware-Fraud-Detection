{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsmaaYassinDev/Behavioural-Anomaly-Detection-for-ATO-Fraud/blob/main/Hybrid_Model_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPd1Wgr28wWe",
        "outputId": "4a4c36d5-8633-40a3-cd92-d6a520fd7efa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"--- Hybrid Model Execution Started (Autoencoder + RF) ---\")\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "# Ensure the file path matches your environment\n",
        "file_path = '/content/drive/My Drive/Colab_Data/PS_20174392719_1491204439457_log.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Data Loaded: {len(df)} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Create Smart Sample & Features ---\n",
        "# This part replicates the feature engineering steps from previous experiments\n",
        "# to ensure consistency.\n",
        "\n",
        "print(\"\\n--- Building Behavioral Profiles (The 'Hard' Features) ---\")\n",
        "# Filter for relevant transaction types\n",
        "df_received = df[df['type'].isin(['TRANSFER', 'CASH_IN'])]\n",
        "df_cashed_out = df[df['type'] == 'CASH_OUT']\n",
        "\n",
        "# Calculate aggregations\n",
        "total_received = df_received.groupby('nameDest')['amount'].sum().to_dict()\n",
        "unique_senders = df_received.groupby('nameDest')['nameOrig'].nunique().to_dict()\n",
        "total_cashed_out = df_cashed_out.groupby('nameOrig')['amount'].sum().to_dict()\n",
        "\n",
        "# Create profiles for all users\n",
        "all_user_ids = set(total_received.keys()) | set(total_cashed_out.keys()) | set(unique_senders.keys())\n",
        "profiles_list = []\n",
        "\n",
        "for user_id in all_user_ids:\n",
        "    received = total_received.get(user_id, 0)\n",
        "    cashed_out = total_cashed_out.get(user_id, 0)\n",
        "    senders = unique_senders.get(user_id, 0)\n",
        "\n",
        "    # dest_cash_out_ratio: How much of received money is cashed out? (Mules ~ 1.0)\n",
        "    ratio = cashed_out / (received + 1e-6)\n",
        "    ratio = min(ratio, 1.0)\n",
        "\n",
        "    profiles_list.append({\n",
        "        'user_id': user_id,\n",
        "        'dest_cash_out_ratio': ratio,\n",
        "        'dest_unique_senders': senders\n",
        "    })\n",
        "\n",
        "final_profiles = pd.DataFrame(profiles_list)\n",
        "print(\"Behavioral profiles built.\")\n",
        "\n",
        "print(\"\\n--- Creating Smart Sample ---\")\n",
        "df_fraud = df[df['isFraud'] == 1]\n",
        "fraud_dest_ids = df_fraud['nameDest'].unique()\n",
        "fraud_orig_ids = df_fraud['nameOrig'].unique()\n",
        "all_fraud_user_ids = np.union1d(fraud_dest_ids, fraud_orig_ids)\n",
        "\n",
        "# Get all users involved in fraud\n",
        "df_fraud_lifecycle = df[\n",
        "    df['nameOrig'].isin(all_fraud_user_ids) |\n",
        "    df['nameDest'].isin(all_fraud_user_ids)\n",
        "]\n",
        "# Sample normal transactions\n",
        "df_normal = df[df['isFraud'] == 0]\n",
        "df_normal_sample = df_normal.sample(n=min(500000, len(df_normal)), random_state=42)\n",
        "\n",
        "df_model_data = pd.concat([df_fraud_lifecycle, df_normal_sample]).drop_duplicates()\n",
        "print(f\"Smart Sample Created: {len(df_model_data)} rows\")\n",
        "\n",
        "# --- 3. Merge Features ---\n",
        "print(\"\\n--- Merging Features ---\")\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameDest', right_on='user_id', how='left')\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameOrig', right_on='user_id', how='left', suffixes=('_dest', '_orig'))\n",
        "\n",
        "for col in ['dest_cash_out_ratio_dest', 'dest_unique_senders_dest', 'dest_cash_out_ratio_orig', 'dest_unique_senders_orig']:\n",
        "    df_model_data[col] = df_model_data[col].fillna(0)\n",
        "\n",
        "# Structural Features (Arithmetic Diffs)\n",
        "df_model_data['balance_diff_orig'] = df_model_data['oldbalanceOrg'] - df_model_data['newbalanceOrig']\n",
        "df_model_data['balance_diff_dest'] = df_model_data['newbalanceDest'] - df_model_data['oldbalanceDest']\n",
        "df_model_data['type_encoded'] = df_model_data['type'].astype('category').cat.codes\n",
        "\n",
        "# --- 5. Prepare Data for Hybrid Model ---\n",
        "# Features that the Autoencoder will use to learn patterns\n",
        "ae_features = [\n",
        "    'amount', 'type_encoded', 'balance_diff_orig', 'balance_diff_dest',\n",
        "    'dest_cash_out_ratio_dest', 'dest_unique_senders_dest'\n",
        "]\n",
        "\n",
        "df_model_data = df_model_data.dropna(subset=['isFraud'])\n",
        "X = df_model_data[ae_features]\n",
        "y = df_model_data['isFraud']\n",
        "\n",
        "# Scale features (important for Neural Networks)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "# --- 6. Phase 1: Train Autoencoder (Feature Extractor) ---\n",
        "print(\"\\n--- Phase 1: Training Autoencoder ---\")\n",
        "# Train only on NORMAL transactions to learn the \"normal\" representation\n",
        "X_train_normal = X_train[y_train == 0]\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "encoder = Dense(16, activation=\"tanh\")(input_layer)\n",
        "encoder = Dense(8, activation=\"relu\")(encoder) # Latent Space representation\n",
        "decoder = Dense(16, activation='tanh')(encoder)\n",
        "decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train for a few epochs\n",
        "autoencoder.fit(X_train_normal, X_train_normal, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# --- 7. Generate \"Anomaly Score\" Feature ---\n",
        "print(\"\\n--- Generating Reconstruction Error Features ---\")\n",
        "# Calculate reconstruction error (MSE) for both train and test sets\n",
        "train_preds = autoencoder.predict(X_train)\n",
        "test_preds = autoencoder.predict(X_test)\n",
        "\n",
        "train_mse = np.mean(np.power(X_train - train_preds, 2), axis=1)\n",
        "test_mse = np.mean(np.power(X_test - test_preds, 2), axis=1)\n",
        "\n",
        "# Add this MSE as a NEW feature to the dataset\n",
        "X_train_hybrid = np.column_stack((X_train, train_mse))\n",
        "X_test_hybrid = np.column_stack((X_test, test_mse))\n",
        "\n",
        "print(f\"New Feature Set Shape: {X_train_hybrid.shape} (Added MSE column)\")\n",
        "\n",
        "# --- 8. Phase 2: Train Random Forest on Hybrid Features ---\n",
        "print(\"\\n--- Phase 2: Training Random Forest on Hybrid Features ---\")\n",
        "# Train RF using the original features PLUS the Autoencoder's anomaly score\n",
        "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100)\n",
        "rf_model.fit(X_train_hybrid, y_train)\n",
        "\n",
        "# --- 9. Evaluate ---\n",
        "y_pred = rf_model.predict(X_test_hybrid)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\n=======================================================\")\n",
        "print(f\"      RESULTS: Hybrid Model (AE + RF)\")\n",
        "print(\"=======================================================\")\n",
        "print(f\"Precision: {precision:.2%}\")\n",
        "print(f\"Recall:    {recall:.2%}\")\n",
        "print(f\"F1-Score:  {f1:.2%}\")\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"=======================================================\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Hybrid Model Execution Started (Autoencoder + RF) ---\n",
            "Data Loaded: 6362620 rows\n",
            "\n",
            "--- Building Behavioral Profiles (The 'Hard' Features) ---\n",
            "Behavioral profiles built.\n",
            "\n",
            "--- Creating Smart Sample ---\n",
            "Smart Sample Created: 561154 rows\n",
            "\n",
            "--- Merging Features ---\n",
            "\n",
            "--- Phase 1: Training Autoencoder ---\n",
            "Epoch 1/5\n",
            "\u001b[1m12096/12096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 0.2995\n",
            "Epoch 2/5\n",
            "\u001b[1m12096/12096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 0.1605\n",
            "Epoch 3/5\n",
            "\u001b[1m12096/12096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 0.0964\n",
            "Epoch 4/5\n",
            "\u001b[1m12096/12096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 0.0785\n",
            "Epoch 5/5\n",
            "\u001b[1m12096/12096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0398\n",
            "\n",
            "--- Generating Reconstruction Error Features ---\n",
            "\u001b[1m12276/12276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step\n",
            "\u001b[1m5261/5261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n",
            "New Feature Set Shape: (392807, 7) (Added MSE column)\n",
            "\n",
            "--- Phase 2: Training Random Forest on Hybrid Features ---\n",
            "\n",
            "=======================================================\n",
            "      RESULTS: Hybrid Model (AE + RF)\n",
            "=======================================================\n",
            "Precision: 87.85%\n",
            "Recall:    75.41%\n",
            "F1-Score:  81.15%\n",
            "-------------------------------------------------------\n",
            "Confusion Matrix:\n",
            "[[165626    257]\n",
            " [   606   1858]]\n",
            "=======================================================\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0rMsWcx8uzi",
        "outputId": "034c1637-48e5-4c61-ef45-3bcef41b3749"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}