{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu5okCgR2eTbYRhp5gx0ey"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UwZxU9_nhCi",
        "outputId": "b252f61f-b575-4ed9-d58d-e2940835aa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Execution Started (Stable Code) ---\n",
            "Objective: Identify Anomalous Accounts based on their behavior.\n",
            "Successfully loaded the full file (83561 rows).\n",
            "\n",
            "--- Step 1: Creating the 'Smart Sample' ---\n",
            "The final 'Smart Sample' was created with 83560 rows.\n",
            "\n",
            "--- Step 2: Building Behavioral Profiles ---\n",
            "Behavioral profiles created successfully.\n",
            "\n",
            "--- Step 3: Merging Features with Transactions ---\n",
            "\n",
            "--- Step 4: Training Isolation Forest Model ---\n",
            "Fraud (Contamination) rate in the Smart Sample: 0.13%\n",
            "Model training complete.\n",
            "\n",
            "--- Step 5: Identifying Anomalous Accounts ---\n",
            "\n",
            "[Final Result]: The model found 149 'anomalous' accounts.\n",
            "Sample of accounts the model considered 'anomalous':\n",
            "['C1017653240' 'C1023714065' 'C1032959800' 'C1041381648' 'C1047512213'\n",
            " 'C1057507014' 'C106297322' 'C1076035261' 'C1090678364' 'C1141137903'\n",
            " 'C1156269888' 'C1171007108' 'C1189372023' 'C12139181' 'C1219553025'\n",
            " 'C1220897602' 'C1225081767' 'C1234776885' 'C1238013097' 'C1240696061']\n",
            "\n",
            "--- For Comparison (The Truth) ---\n",
            "The number of 'real' fraudulent accounts (isFraud=1) was: 223\n",
            "Sample of 'real' fraudulent accounts:\n",
            "['C1002031672' 'C1004271827' 'C1007251739' 'C1013511446' 'C1020461364'\n",
            " 'C1023505879' 'C102617052' 'C1026280121' 'C1032986144' 'C1034219836'\n",
            " 'C1034673425' 'C1060999444' 'C1087133093' 'C1093223281' 'C1101441872'\n",
            " 'C1118430673' 'C1121789613' 'C1134864869' 'C1136419747' 'C116289363']\n",
            "\n",
            "The model's F1-Score (for confirmation): 2.68%\n",
            "\n",
            "--- Stable Code Execution Complete ---\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "\n",
        "# Ignore unimportant warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"--- Execution Started (Stable Code) ---\")\n",
        "print(\"Objective: Identify Anomalous Accounts based on their behavior.\")\n",
        "\n",
        "# --- Load Data (Make sure this name matches the file you uploaded) ---\n",
        "file_path = 'PS_20174392719_1491204439457_log.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Successfully loaded the full file ({len(df)} rows).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading: {e}\")\n",
        "    print(\"!!! Make sure the file name in the code (file_path) matches the file in the folder exactly !!!\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 1: Create the 'Smart Sample' ---\n",
        "print(\"\\n--- Step 1: Creating the 'Smart Sample' ---\")\n",
        "df_fraud = df[df['isFraud'] == 1]\n",
        "fraud_dest_ids = df_fraud['nameDest'].unique()\n",
        "fraud_orig_ids = df_fraud['nameOrig'].unique()\n",
        "all_fraud_user_ids = np.union1d(fraud_dest_ids, fraud_orig_ids)\n",
        "df_fraud_lifecycle = df[\n",
        "    df['nameOrig'].isin(all_fraud_user_ids) |\n",
        "    df['nameDest'].isin(all_fraud_user_ids)\n",
        "]\n",
        "\n",
        "df_normal = df[df['isFraud'] == 0]\n",
        "sample_size = min(500000, len(df_normal))\n",
        "df_normal_sample = df_normal.sample(n=sample_size, random_state=42)\n",
        "\n",
        "df_smart_sample = pd.concat([df_fraud_lifecycle, df_normal_sample]).drop_duplicates(keep='first')\n",
        "print(f\"The final 'Smart Sample' was created with {len(df_smart_sample)} rows.\")\n",
        "\n",
        "# --- Step 2: Build Behavioral Profiles (Strong Features) ---\n",
        "print(\"\\n--- Step 2: Building Behavioral Profiles ---\")\n",
        "\n",
        "# (a) Calculate total received, total cashed out, and unique senders count\n",
        "df_received = df_smart_sample[df_smart_sample['type'].isin(['TRANSFER', 'CASH_IN'])]\n",
        "total_received = df_received.groupby('nameDest')['amount'].sum().to_dict()\n",
        "unique_senders = df_received.groupby('nameDest')['nameOrig'].nunique().to_dict()\n",
        "\n",
        "df_cashed_out = df_smart_sample[df_smart_sample['type'] == 'CASH_OUT']\n",
        "total_cashed_out = df_cashed_out.groupby('nameOrig')['amount'].sum().to_dict()\n",
        "\n",
        "all_user_ids = set(total_received.keys()) | set(total_cashed_out.keys()) | set(unique_senders.keys())\n",
        "profiles_list = []\n",
        "for user_id in all_user_ids:\n",
        "    received = total_received.get(user_id, 0)\n",
        "    cashed_out = total_cashed_out.get(user_id, 0)\n",
        "    senders = unique_senders.get(user_id, 0)\n",
        "\n",
        "    ratio = (cashed_out / (received + 1e-6))\n",
        "    ratio = min(ratio, 1.0) # The ratio cannot exceed 100%\n",
        "\n",
        "    profiles_list.append({\n",
        "        'user_id': user_id,\n",
        "        'dest_cash_out_ratio': ratio,\n",
        "        'dest_unique_senders': senders\n",
        "    })\n",
        "\n",
        "final_profiles = pd.DataFrame(profiles_list)\n",
        "print(\"Behavioral profiles created successfully.\")\n",
        "\n",
        "# --- Step 3: Merge Features with Transactions ---\n",
        "print(\"\\n--- Step 3: Merging Features with Transactions ---\")\n",
        "df_model_data = pd.merge(df_smart_sample, final_profiles, left_on='nameDest', right_on='user_id', how='left')\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameOrig', right_on='user_id', how='left', suffixes=('_dest', '_orig'))\n",
        "\n",
        "df_model_data['dest_cash_out_ratio_dest'] = df_model_data['dest_cash_out_ratio_dest'].fillna(0)\n",
        "df_model_data['dest_unique_senders_dest'] = df_model_data['dest_unique_senders_dest'].fillna(0)\n",
        "df_model_data['dest_cash_out_ratio_orig'] = df_model_data['dest_cash_out_ratio_orig'].fillna(0)\n",
        "df_model_data['dest_unique_senders_orig'] = df_model_data['dest_unique_senders_orig'].fillna(0)\n",
        "\n",
        "# --- Step 4: Train an Unsupervised Model ---\n",
        "print(\"\\n--- Step 4: Training Isolation Forest Model ---\")\n",
        "\n",
        "features = [\n",
        "    'amount',\n",
        "    'dest_cash_out_ratio_dest', # Recipient's cash-out ratio\n",
        "    'dest_unique_senders_dest', # Recipient's unique senders\n",
        "    'dest_cash_out_ratio_orig', # Sender's cash-out ratio\n",
        "    'dest_unique_senders_orig'  # Sender's unique senders\n",
        "]\n",
        "df_model_data['type_encoded'] = df_model_data['type'].astype('category').cat.codes\n",
        "features.append('type_encoded')\n",
        "\n",
        "X = df_model_data[features]\n",
        "y_true = df_model_data['isFraud'] # The \"Correct Answer\"\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "contamination = y_true.mean()\n",
        "print(f\"Fraud (Contamination) rate in the Smart Sample: {contamination:.2%}\")\n",
        "\n",
        "model = IsolationForest(contamination=contamination, random_state=42)\n",
        "model.fit(X_scaled)\n",
        "predictions = model.predict(X_scaled) # -1 = anomalous, 1 = normal\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 5: Answer Your Question ---\n",
        "print(\"\\n--- Step 5: Identifying Anomalous Accounts ---\")\n",
        "\n",
        "# Add the model's \"guess\" to the data\n",
        "# 1 = anomalous, 0 = normal\n",
        "df_model_data['anomaly_prediction'] = [1 if p == -1 else 0 for p in predictions]\n",
        "\n",
        "# Find the \"transactions\" that the model judged as anomalous\n",
        "anomalous_transactions = df_model_data[df_model_data['anomaly_prediction'] == 1]\n",
        "\n",
        "# Find the \"account names\" (senders and recipients) involved in these anomalous transactions\n",
        "anomalous_dest_accounts = anomalous_transactions['nameDest'].unique()\n",
        "anomalous_orig_accounts = anomalous_transactions['nameOrig'].unique()\n",
        "\n",
        "all_anomalous_accounts = np.union1d(anomalous_dest_accounts, anomalous_orig_accounts)\n",
        "\n",
        "print(f\"\\n[Final Result]: The model found {len(all_anomalous_accounts)} 'anomalous' accounts.\")\n",
        "\n",
        "# Print a sample of 20 accounts the model considered 'anomalous'\n",
        "print(\"Sample of accounts the model considered 'anomalous':\")\n",
        "print(all_anomalous_accounts[:20])\n",
        "\n",
        "# --- For Comparison: What are the \"Real\" Fraudulent Accounts? ---\n",
        "print(\"\\n--- For Comparison (The Truth) ---\")\n",
        "print(f\"The number of 'real' fraudulent accounts (isFraud=1) was: {len(all_fraud_user_ids)}\")\n",
        "print(\"Sample of 'real' fraudulent accounts:\")\n",
        "print(all_fraud_user_ids[:20])\n",
        "\n",
        "# Calculate F1-Score to verify quality\n",
        "f1 = f1_score(y_true, df_model_data['anomaly_prediction'])\n",
        "print(f\"\\nThe model's F1-Score (for confirmation): {f1:.2%}\")\n",
        "\n",
        "print(\"\\n--- Stable Code Execution Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "\n",
        "# Ignore unimportant warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"--- Execution Started (Stable Code) ---\")\n",
        "print(\"Objective: Identify Anomalous Accounts based on their behavior.\")\n",
        "\n",
        "# --- Load Data (Make sure this name matches the file you uploaded) ---\n",
        "file_path = 'PS_20174392719_1491204439457_log.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Successfully loaded the full file ({len(df)} rows).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading: {e}\")\n",
        "    print(\"!!! Make sure the file name in the code (file_path) matches the file in the folder exactly !!!\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 1: Create the 'Smart Sample' ---\n",
        "print(\"\\n--- Step 1: Creating the 'Smart Sample' ---\")\n",
        "df_fraud = df[df['isFraud'] == 1]\n",
        "fraud_dest_ids = df_fraud['nameDest'].unique()\n",
        "fraud_orig_ids = df_fraud['nameOrig'].unique()\n",
        "all_fraud_user_ids = np.union1d(fraud_dest_ids, fraud_orig_ids)\n",
        "df_fraud_lifecycle = df[\n",
        "    df['nameOrig'].isin(all_fraud_user_ids) |\n",
        "    df['nameDest'].isin(all_fraud_user_ids)\n",
        "]\n",
        "\n",
        "df_normal = df[df['isFraud'] == 0]\n",
        "sample_size = min(500000, len(df_normal))\n",
        "df_normal_sample = df_normal.sample(n=sample_size, random_state=42)\n",
        "\n",
        "df_smart_sample = pd.concat([df_fraud_lifecycle, df_normal_sample]).drop_duplicates(keep='first')\n",
        "print(f\"The final 'Smart Sample' was created with {len(df_smart_sample)} rows.\")\n",
        "\n",
        "# --- Step 2: Build Behavioral Profiles (Strong Features) ---\n",
        "print(\"\\n--- Step 2: Building Behavioral Profiles ---\")\n",
        "\n",
        "# (a) Calculate total received, total cashed out, and unique senders count\n",
        "df_received = df_smart_sample[df_smart_sample['type'].isin(['TRANSFER', 'CASH_IN'])]\n",
        "total_received = df_received.groupby('nameDest')['amount'].sum().to_dict()\n",
        "unique_senders = df_received.groupby('nameDest')['nameOrig'].nunique().to_dict()\n",
        "\n",
        "df_cashed_out = df_smart_sample[df_smart_sample['type'] == 'CASH_OUT']\n",
        "total_cashed_out = df_cashed_out.groupby('nameOrig')['amount'].sum().to_dict()\n",
        "\n",
        "all_user_ids = set(total_received.keys()) | set(total_cashed_out.keys()) | set(unique_senders.keys())\n",
        "profiles_list = []\n",
        "for user_id in all_user_ids:\n",
        "    received = total_received.get(user_id, 0)\n",
        "    cashed_out = total_cashed_out.get(user_id, 0)\n",
        "    senders = unique_senders.get(user_id, 0)\n",
        "\n",
        "    ratio = (cashed_out / (received + 1e-6))\n",
        "    ratio = min(ratio, 1.0) # The ratio cannot exceed 100%\n",
        "\n",
        "    profiles_list.append({\n",
        "        'user_id': user_id,\n",
        "        'dest_cash_out_ratio': ratio,\n",
        "        'dest_unique_senders': senders\n",
        "    })\n",
        "\n",
        "final_profiles = pd.DataFrame(profiles_list)\n",
        "print(\"Behavioral profiles created successfully.\")\n",
        "\n",
        "# --- Step 3: Merge Features with Transactions ---\n",
        "print(\"\\n--- Step 3: Merging Features with Transactions ---\")\n",
        "df_model_data = pd.merge(df_smart_sample, final_profiles, left_on='nameDest', right_on='user_id', how='left')\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameOrig', right_on='user_id', how='left', suffixes=('_dest', '_orig'))\n",
        "\n",
        "df_model_data['dest_cash_out_ratio_dest'] = df_model_data['dest_cash_out_ratio_dest'].fillna(0)\n",
        "df_model_data['dest_unique_senders_dest'] = df_model_data['dest_unique_senders_dest'].fillna(0)\n",
        "df_model_data['dest_cash_out_ratio_orig'] = df_model_data['dest_cash_out_ratio_orig'].fillna(0)\n",
        "df_model_data['dest_unique_senders_orig'] = df_model_data['dest_unique_senders_orig'].fillna(0)\n",
        "\n",
        "# --- Step 4: Train an Unsupervised Model ---\n",
        "print(\"\\n--- Step 4: Training Isolation Forest Model ---\")\n",
        "\n",
        "features = [\n",
        "    'amount',\n",
        "    'dest_cash_out_ratio_dest', # Recipient's cash-out ratio\n",
        "    'dest_unique_senders_dest', # Recipient's unique senders\n",
        "    'dest_cash_out_ratio_orig', # Sender's cash-out ratio\n",
        "    'dest_unique_senders_orig'  # Sender's unique senders\n",
        "]\n",
        "df_model_data['type_encoded'] = df_model_data['type'].astype('category').cat.codes\n",
        "features.append('type_encoded')\n",
        "\n",
        "X = df_model_data[features]\n",
        "y_true = df_model_data['isFraud'] # The \"Correct Answer\"\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "contamination = y_true.mean()\n",
        "print(f\"Fraud (Contamination) rate in the Smart Sample: {contamination:.2%}\")\n",
        "\n",
        "model = IsolationForest(contamination=contamination, random_state=42)\n",
        "model.fit(X_scaled)\n",
        "predictions = model.predict(X_scaled) # -1 = anomalous, 1 = normal\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 5: Answer Your Question ---\n",
        "print(\"\\n--- Step 5: Identifying Anomalous Accounts ---\")\n",
        "\n",
        "# Add the model's \"guess\" to the data\n",
        "# 1 = anomalous, 0 = normal\n",
        "df_model_data['anomaly_prediction'] = [1 if p == -1 else 0 for p in predictions]\n",
        "\n",
        "# Find the \"transactions\" that the model judged as anomalous\n",
        "anomalous_transactions = df_model_data[df_model_data['anomaly_prediction'] == 1]\n",
        "\n",
        "# Find the \"account names\" (senders and recipients) involved in these anomalous transactions\n",
        "anomalous_dest_accounts = anomalous_transactions['nameDest'].unique()\n",
        "anomalous_orig_accounts = anomalous_transactions['nameOrig'].unique()\n",
        "\n",
        "all_anomalous_accounts = np.union1d(anomalous_dest_accounts, anomalous_orig_accounts)\n",
        "\n",
        "print(f\"\\n[Final Result]: The model found {len(all_anomalous_accounts)} 'anomalous' accounts.\")\n",
        "\n",
        "# Print a sample of 20 accounts the model considered 'anomalous'\n",
        "print(\"Sample of accounts the model considered 'anomalous':\")\n",
        "print(all_anomalous_accounts[:20])\n",
        "\n",
        "# --- For Comparison: What are the \"Real\" Fraudulent Accounts? ---\n",
        "print(\"\\n--- For Comparison (The Truth) ---\")\n",
        "print(f\"The number of 'real' fraudulent accounts (isFraud=1) was: {len(all_fraud_user_ids)}\")\n",
        "print(\"Sample of 'real' fraudulent accounts:\")\n",
        "print(all_fraud_user_ids[:20])\n",
        "\n",
        "# Calculate F1-Score to verify quality\n",
        "f1 = f1_score(y_true, df_model_data['anomaly_prediction'])\n",
        "print(f\"\\nThe model's F1-Score (for confirmation): {f1:.2%}\")\n",
        "\n",
        "print(\"\\n--- Stable Code Execution Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfkho9rksQUp",
        "outputId": "8f304c12-f00f-4bd9-92fd-5ef9d0ec4771"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Execution Started (Stable Code) ---\n",
            "Objective: Identify Anomalous Accounts based on their behavior.\n",
            "Successfully loaded the full file (6362620 rows).\n",
            "\n",
            "--- Step 1: Creating the 'Smart Sample' ---\n",
            "The final 'Smart Sample' was created with 561154 rows.\n",
            "\n",
            "--- Step 2: Building Behavioral Profiles ---\n",
            "Behavioral profiles created successfully.\n",
            "\n",
            "--- Step 3: Merging Features with Transactions ---\n",
            "\n",
            "--- Step 4: Training Isolation Forest Model ---\n",
            "Fraud (Contamination) rate in the Smart Sample: 1.46%\n",
            "Model training complete.\n",
            "\n",
            "--- Step 5: Identifying Anomalous Accounts ---\n",
            "\n",
            "[Final Result]: The model found 11376 'anomalous' accounts.\n",
            "Sample of accounts the model considered 'anomalous':\n",
            "['C1000156006' 'C1000484178' 'C1000628778' 'C1000868784' 'C1001249070'\n",
            " 'C1001476563' 'C1001658373' 'C1001742979' 'C1001844551' 'C1002031672'\n",
            " 'C1002055980' 'C1002446735' 'C1002469873' 'C1003024596' 'C1003280328'\n",
            " 'C1003526443' 'C1003663195' 'C100367356' 'C1003909463' 'C100394411']\n",
            "\n",
            "--- For Comparison (The Truth) ---\n",
            "The number of 'real' fraudulent accounts (isFraud=1) was: 16382\n",
            "Sample of 'real' fraudulent accounts:\n",
            "['C1000036340' 'C1000039615' 'C1000086512' 'C1000331499' 'C1000367306'\n",
            " 'C1000407130' 'C1000484178' 'C1000513158' 'C1000836187' 'C1000855680'\n",
            " 'C1000868784' 'C1000937208' 'C1001258143' 'C1001502110' 'C1001664465'\n",
            " 'C1001765380' 'C1001875185' 'C1002031672' 'C1002151702' 'C1002183297']\n",
            "\n",
            "The model's F1-Score (for confirmation): 19.16%\n",
            "\n",
            "--- Stable Code Execution Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# --- (Fix: Imported the correct model name) ---\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "# Ignore unimportant warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"--- Execution Started (Smartest Code: With 'Time' Feature) ---\")\n",
        "print(\"Objective: Improve F1-Score by adding 'avg_time_to_cash_out'.\")\n",
        "\n",
        "# --- Load Data ---\n",
        "# Make sure this name matches the file you uploaded\n",
        "file_path = 'PS_20174392719_1491204439457_log.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Successfully loaded the full file ({len(df)} rows).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 1: Create the 'Smart Sample' ---\n",
        "print(\"\\n--- Step 1: Creating the 'Smart Sample' ---\")\n",
        "df_fraud = df[df['isFraud'] == 1]\n",
        "fraud_dest_ids = df_fraud['nameDest'].unique()\n",
        "fraud_orig_ids = df_fraud['nameOrig'].unique()\n",
        "all_fraud_user_ids = np.union1d(fraud_dest_ids, fraud_orig_ids)\n",
        "df_fraud_lifecycle = df[\n",
        "    df['nameOrig'].isin(all_fraud_user_ids) |\n",
        "    df['nameDest'].isin(all_fraud_user_ids)\n",
        "]\n",
        "df_normal = df[df['isFraud'] == 0]\n",
        "sample_size = min(500000, len(df_normal))\n",
        "df_normal_sample = df_normal.sample(n=sample_size, random_state=42)\n",
        "df_smart_sample = pd.concat([df_fraud_lifecycle, df_normal_sample]).drop_duplicates(keep='first')\n",
        "print(f\"The final 'Smart Sample' was created with {len(df_smart_sample)} rows.\")\n",
        "\n",
        "# --- Step 2: Building Behavioral Profiles (Including 'Time') ---\n",
        "print(\"\\n--- Step 2: Building Behavioral Profiles (Full) ---\")\n",
        "\n",
        "# (a) Calculate basic stats (Ratio, Sender Count)\n",
        "print(\"   (a) Calculating cash-out ratio and unique senders...\")\n",
        "df_received = df_smart_sample[df_smart_sample['type'].isin(['TRANSFER', 'CASH_IN'])]\n",
        "total_received = df_received.groupby('nameDest')['amount'].sum().to_dict()\n",
        "unique_senders = df_received.groupby('nameDest')['nameOrig'].nunique().to_dict()\n",
        "df_cashed_out = df_smart_sample[df_smart_sample['type'] == 'CASH_OUT']\n",
        "total_cashed_out = df_cashed_out.groupby('nameOrig')['amount'].sum().to_dict()\n",
        "\n",
        "all_user_ids = set(total_received.keys()) | set(total_cashed_out.keys()) | set(unique_senders.keys())\n",
        "profiles_list = []\n",
        "for user_id in all_user_ids:\n",
        "    received = total_received.get(user_id, 0)\n",
        "    cashed_out = total_cashed_out.get(user_id, 0)\n",
        "    senders = unique_senders.get(user_id, 0)\n",
        "    ratio = (cashed_out / (received + 1e-6))\n",
        "    ratio = min(ratio, 1.0)\n",
        "    profiles_list.append({\n",
        "        'user_id': user_id,\n",
        "        'dest_cash_out_ratio': ratio,\n",
        "        'dest_unique_senders': senders\n",
        "    })\n",
        "final_profiles = pd.DataFrame(profiles_list)\n",
        "\n",
        "# (b) Calculate average time to cash-out (the smartest feature using 'step')\n",
        "print(\"   (b) Calculating average time to cash-out (avg_time_to_cash_out)...\")\n",
        "df_transfers = df_smart_sample[df_smart_sample['type'] == 'TRANSFER'][['step', 'nameDest']]\n",
        "df_cashouts = df_smart_sample[df_smart_sample['type'] == 'CASH_OUT'][['step', 'nameOrig']]\n",
        "df_transfers.rename(columns={'nameDest': 'user_id'}, inplace=True)\n",
        "df_cashouts.rename(columns={'nameOrig': 'user_id'}, inplace=True)\n",
        "df_transfers['tx_type'] = 'TRANSFER_IN'\n",
        "df_cashouts['tx_type'] = 'CASH_OUT'\n",
        "\n",
        "user_log = pd.concat([df_transfers, df_cashouts]).sort_values(by=['user_id', 'step'])\n",
        "user_log['prev_step'] = user_log.groupby('user_id')['step'].shift(1)\n",
        "user_log['prev_type'] = user_log.groupby('user_id')['tx_type'].shift(1)\n",
        "\n",
        "# (Correcting the previous error)\n",
        "user_log['time_since_transfer'] = user_log['step'] - user_log['prev_step']\n",
        "is_pattern = (user_log['tx_type'] == 'CASH_OUT') & (user_log['prev_type'] == 'TRANSFER_IN')\n",
        "pattern_times = user_log[is_pattern]\n",
        "\n",
        "avg_time_profile = pattern_times.groupby('user_id')['time_since_transfer'].mean().reset_index()\n",
        "avg_time_profile.columns = ['user_id', 'avg_time_to_cash_out']\n",
        "\n",
        "# (c) Aggregate final profiles\n",
        "print(\"   (c) Aggregating final profiles...\")\n",
        "final_profiles = pd.merge(final_profiles, avg_time_profile, on='user_id', how='left')\n",
        "# Fill NaNs: If the user didn't follow the pattern, set a long time (e.g., 999)\n",
        "final_profiles['avg_time_to_cash_out'] = final_profiles['avg_time_to_cash_out'].fillna(999)\n",
        "print(\"Complex behavioral profiles created successfully.\")\n",
        "\n",
        "# --- Step 3: Merge Features with Transactions ---\n",
        "print(\"\\n--- Step 3: Merging Features with Transactions ---\")\n",
        "df_model_data = pd.merge(df_smart_sample, final_profiles, left_on='nameDest', right_on='user_id', how='left')\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameOrig', right_on='user_id', how='left', suffixes=('_dest', '_orig'))\n",
        "\n",
        "# Fill NaNs from the merge\n",
        "df_model_data['dest_cash_out_ratio_dest'] = df_model_data['dest_cash_out_ratio_dest'].fillna(0)\n",
        "df_model_data['dest_unique_senders_dest'] = df_model_data['dest_unique_senders_dest'].fillna(0)\n",
        "df_model_data['avg_time_to_cash_out_dest'] = df_model_data['avg_time_to_cash_out_dest'].fillna(999)\n",
        "df_model_data['dest_cash_out_ratio_orig'] = df_model_data['dest_cash_out_ratio_orig'].fillna(0)\n",
        "df_model_data['dest_unique_senders_orig'] = df_model_data['dest_unique_senders_orig'].fillna(0)\n",
        "df_model_data['avg_time_to_cash_out_orig'] = df_model_data['avg_time_to_cash_out_orig'].fillna(999)\n",
        "\n",
        "# --- Step 4: Train Unsupervised Model ---\n",
        "print(\"\\n--- Step 4: Training Isolation Forest Model (With New Features) ---\")\n",
        "\n",
        "features = [\n",
        "    'amount',\n",
        "    'dest_cash_out_ratio_dest',\n",
        "    'dest_unique_senders_dest',\n",
        "    'avg_time_to_cash_out_dest', # <-- New feature (for recipient)\n",
        "    'dest_cash_out_ratio_orig',\n",
        "    'dest_unique_senders_orig',\n",
        "    'avg_time_to_cash_out_orig'  # <-- New feature (for sender)\n",
        "]\n",
        "df_model_data['type_encoded'] = df_model_data['type'].astype('category').cat.codes\n",
        "features.append('type_encoded')\n",
        "\n",
        "X = df_model_data[features]\n",
        "y_true = df_model_data['isFraud'] # The \"Correct Answer\" (for evaluation only)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "contamination = y_true.mean()\n",
        "print(f\"Fraud (Contamination) rate in the Smart Sample: {contamination:.2%}\")\n",
        "\n",
        "# --- (This is the corrected line) ---\n",
        "model = IsolationForest(contamination=contamination, random_state=42)\n",
        "model.fit(X_scaled)\n",
        "predictions = model.predict(X_scaled) # -1 = anomalous, 1 = normal\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 5: Evaluate the Model (Comparison) ---\n",
        "print(\"\\n--- Step 5: Evaluating the Model (New Score) ---\")\n",
        "\n",
        "y_pred = [1 if p == -1 else 0 for p in predictions]\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.2%}\")\n",
        "print(f\"Recall: {recall:.2%}\")\n",
        "print(f\"F1-Score (New Final Score): {f1:.2%}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"\\n--- Smartest Code Execution Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnupagTnpQYT",
        "outputId": "5038ea78-7836-4677-d061-18f854bfc96e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Execution Started (Smartest Code: With 'Time' Feature) ---\n",
            "Objective: Improve F1-Score by adding 'avg_time_to_cash_out'.\n",
            "Successfully loaded the full file (192593 rows).\n",
            "\n",
            "--- Step 1: Creating the 'Smart Sample' ---\n",
            "The final 'Smart Sample' was created with 192592 rows.\n",
            "\n",
            "--- Step 2: Building Behavioral Profiles (Full) ---\n",
            "   (a) Calculating cash-out ratio and unique senders...\n",
            "   (b) Calculating average time to cash-out (avg_time_to_cash_out)...\n",
            "   (c) Aggregating final profiles...\n",
            "Complex behavioral profiles created successfully.\n",
            "\n",
            "--- Step 3: Merging Features with Transactions ---\n",
            "\n",
            "--- Step 4: Training Isolation Forest Model (With New Features) ---\n",
            "Fraud (Contamination) rate in the Smart Sample: 0.07%\n",
            "Model training complete.\n",
            "\n",
            "--- Step 5: Evaluating the Model (New Score) ---\n",
            "Precision: 2.10%\n",
            "Recall: 2.10%\n",
            "F1-Score (New Final Score): 2.10%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[192309    140]\n",
            " [   140      3]]\n",
            "\n",
            "--- Smartest Code Execution Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# --- (Change: We will use a Supervised model) ---\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "# --- (Change: We need to split the data) ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n",
        "# Ignore unimportant warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"--- Execution Started (Successful Model: Supervised) ---\")\n",
        "print(\"Objective: Prove that 'Behavioral Features' succeed with a Supervised model.\")\n",
        "\n",
        "# --- Load Data ---\n",
        "file_path = 'PS_20174392719_1491204439457_log.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Successfully loaded the full file ({len(df)} rows).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 1: Create the 'Smart Sample' ---\n",
        "# (Same code to build a balanced sample for training)\n",
        "print(\"\\n--- Step 1: Creating the 'Smart Sample' ---\")\n",
        "df_fraud = df[df['isFraud'] == 1]\n",
        "fraud_dest_ids = df_fraud['nameDest'].unique()\n",
        "fraud_orig_ids = df_fraud['nameOrig'].unique()\n",
        "all_fraud_user_ids = np.union1d(fraud_dest_ids, fraud_orig_ids)\n",
        "df_fraud_lifecycle = df[\n",
        "    df['nameOrig'].isin(all_fraud_user_ids) |\n",
        "    df['nameDest'].isin(all_fraud_user_ids)\n",
        "]\n",
        "df_normal = df[df['isFraud'] == 0]\n",
        "sample_size = min(500000, len(df_normal))\n",
        "df_normal_sample = df_normal.sample(n=sample_size, random_state=42)\n",
        "df_smart_sample = pd.concat([df_fraud_lifecycle, df_normal_sample]).drop_duplicates(keep='first')\n",
        "print(f\"The final 'Smart Sample' was created with {len(df_smart_sample)} rows.\")\n",
        "\n",
        "# --- Step 2: Building Behavioral Profiles (Strong Features) ---\n",
        "print(\"\\n--- Step 2: Building Behavioral Profiles ---\")\n",
        "df_received = df_smart_sample[df_smart_sample['type'].isin(['TRANSFER', 'CASH_IN'])]\n",
        "total_received = df_received.groupby('nameDest')['amount'].sum().to_dict()\n",
        "unique_senders = df_received.groupby('nameDest')['nameOrig'].nunique().to_dict()\n",
        "df_cashed_out = df_smart_sample[df_smart_sample['type'] == 'CASH_OUT']\n",
        "total_cashed_out = df_cashed_out.groupby('nameOrig')['amount'].sum().to_dict()\n",
        "\n",
        "all_user_ids = set(total_received.keys()) | set(total_cashed_out.keys()) | set(unique_senders.keys())\n",
        "profiles_list = []\n",
        "for user_id in all_user_ids:\n",
        "    received = total_received.get(user_id, 0)\n",
        "    cashed_out = total_cashed_out.get(user_id, 0)\n",
        "    senders = unique_senders.get(user_id, 0)\n",
        "    ratio = (cashed_out / (received + 1e-6))\n",
        "    ratio = min(ratio, 1.0)\n",
        "    profiles_list.append({\n",
        "        'user_id': user_id,\n",
        "        'dest_cash_out_ratio': ratio,\n",
        "        'dest_unique_senders': senders\n",
        "    })\n",
        "final_profiles = pd.DataFrame(profiles_list)\n",
        "print(\"Behavioral profiles created successfully.\")\n",
        "\n",
        "# --- Step 3: Merge Features with Transactions ---\n",
        "print(\"\\n--- Step 3: Merging Features with Transactions ---\")\n",
        "df_model_data = pd.merge(df_smart_sample, final_profiles, left_on='nameDest', right_on='user_id', how='left')\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameOrig', right_on='user_id', how='left', suffixes=('_dest', '_orig'))\n",
        "\n",
        "df_model_data['dest_cash_out_ratio_dest'] = df_model_data['dest_cash_out_ratio_dest'].fillna(0)\n",
        "df_model_data['dest_unique_senders_dest'] = df_model_data['dest_unique_senders_dest'].fillna(0)\n",
        "df_model_data['dest_cash_out_ratio_orig'] = df_model_data['dest_cash_out_ratio_orig'].fillna(0)\n",
        "df_model_data['dest_unique_senders_orig'] = df_model_data['dest_unique_senders_orig'].fillna(0)\n",
        "\n",
        "# --- Step 4: Prepare Train/Test Data (Supervised) ---\n",
        "print(\"\\n--- Step 4: Preparing Train/Test Data ---\")\n",
        "\n",
        "features = [\n",
        "    'amount',\n",
        "    'dest_cash_out_ratio_dest', # Recipient's cash-out ratio\n",
        "    'dest_unique_senders_dest', # Recipient's unique senders\n",
        "    'dest_cash_out_ratio_orig', # Sender's cash-out ratio\n",
        "    'dest_unique_senders_orig'  # Sender's unique senders\n",
        "]\n",
        "df_model_data['type_encoded'] = df_model_data['type'].astype('category').cat.codes\n",
        "features.append('type_encoded')\n",
        "\n",
        "X = df_model_data[features]\n",
        "y_true = df_model_data['isFraud'] # The \"Correct Answer\"\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# (Change: We split the data 70% for training and 30% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_true, test_size=0.3, random_state=42, stratify=y_true)\n",
        "print(f\"Data split into {len(X_train)} rows for training and {len(X_test)} rows for testing.\")\n",
        "\n",
        "# --- Step 5: Train Supervised RandomForest Model ---\n",
        "print(\"\\n--- Step 5: Training RandomForest Model (Supervised) ---\")\n",
        "\n",
        "# (class_weight='balanced' is very important for imbalanced data)\n",
        "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Test on \"new\" data (X_test) that the model has never seen before\n",
        "predictions = model.predict(X_test)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 6: Evaluate (The Successful Result) ---\n",
        "print(\"\\n--- Step 6: Evaluating the Model (New Result) ---\")\n",
        "\n",
        "f1 = f1_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(\"!!! Results on 'Test Data' (data the model has never seen before) !!!\")\n",
        "print(f\"Precision: {precision:.2%}\")\n",
        "print(f\"Recall: {recall:.2%}\")\n",
        "print(f\"F1-Score (New Final Score): {f1:.2%}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# --- (The most important step: Why did the model succeed?) ---\n",
        "print(\"\\n--- Feature Importance (Why did the model succeed?) ---\")\n",
        "feature_imp = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
        "print(feature_imp)\n",
        "\n",
        "print(\"\\n--- Successful Code Execution Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDaSHX02rhbW",
        "outputId": "84a35c68-00ab-464a-e9a1-ea569bcac844"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Execution Started (Successful Model: Supervised) ---\n",
            "Objective: Prove that 'Behavioral Features' succeed with a Supervised model.\n",
            "Successfully loaded the full file (355422 rows).\n",
            "\n",
            "--- Step 1: Creating the 'Smart Sample' ---\n",
            "The final 'Smart Sample' was created with 355421 rows.\n",
            "\n",
            "--- Step 2: Building Behavioral Profiles ---\n",
            "Behavioral profiles created successfully.\n",
            "\n",
            "--- Step 3: Merging Features with Transactions ---\n",
            "\n",
            "--- Step 4: Preparing Train/Test Data ---\n",
            "Data split into 248794 rows for training and 106627 rows for testing.\n",
            "\n",
            "--- Step 5: Training RandomForest Model (Supervised) ---\n",
            "Model training complete.\n",
            "\n",
            "--- Step 6: Evaluating the Model (New Result) ---\n",
            "!!! Results on 'Test Data' (data the model has never seen before) !!!\n",
            "Precision: 20.00%\n",
            "Recall: 8.47%\n",
            "F1-Score (New Final Score): 11.90%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[106548     20]\n",
            " [    54      5]]\n",
            "\n",
            "--- Feature Importance (Why did the model succeed?) ---\n",
            "amount                      0.451471\n",
            "type_encoded                0.251712\n",
            "dest_unique_senders_dest    0.216736\n",
            "dest_cash_out_ratio_orig    0.080080\n",
            "dest_unique_senders_orig    0.000001\n",
            "dest_cash_out_ratio_dest    0.000000\n",
            "dtype: float64\n",
            "\n",
            "--- Successful Code Execution Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# --- (Change: We will use LOF) ---\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "# Ignore unimportant warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"--- Execution Started (Comparison with LOF) ---\")\n",
        "print(\"Objective: Test LOF's performance on the same behavioral features.\")\n",
        "\n",
        "# --- Load Data ---\n",
        "file_path = 'PS_20174392719_1491204439457_log.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Successfully loaded the full file ({len(df)} rows).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 1: Create the 'Smart Sample' ---\n",
        "print(\"\\n--- Step 1: Creating the 'Smart Sample' ---\")\n",
        "df_fraud = df[df['isFraud'] == 1]\n",
        "fraud_dest_ids = df_fraud['nameDest'].unique()\n",
        "fraud_orig_ids = df_fraud['nameOrig'].unique()\n",
        "all_fraud_user_ids = np.union1d(fraud_dest_ids, fraud_orig_ids)\n",
        "df_fraud_lifecycle = df[\n",
        "    df['nameOrig'].isin(all_fraud_user_ids) |\n",
        "    df['nameDest'].isin(all_fraud_user_ids)\n",
        "]\n",
        "df_normal = df[df['isFraud'] == 0]\n",
        "sample_size = min(500000, len(df_normal))\n",
        "df_normal_sample = df_normal.sample(n=sample_size, random_state=42)\n",
        "df_smart_sample = pd.concat([df_fraud_lifecycle, df_normal_sample]).drop_duplicates(keep='first')\n",
        "print(f\"The final 'Smart Sample' was created with {len(df_smart_sample)} rows.\")\n",
        "\n",
        "# --- Step 2: Build Behavioral Profiles (Strong Features) ---\n",
        "print(\"\\n--- Step 2: Building Behavioral Profiles ---\")\n",
        "\n",
        "# (a) Calculate basic statistics\n",
        "df_received = df_smart_sample[df_smart_sample['type'].isin(['TRANSFER', 'CASH_IN'])]\n",
        "total_received = df_received.groupby('nameDest')['amount'].sum().to_dict()\n",
        "unique_senders = df_received.groupby('nameDest')['nameOrig'].nunique().to_dict()\n",
        "df_cashed_out = df_smart_sample[df_smart_sample['type'] == 'CASH_OUT']\n",
        "total_cashed_out = df_cashed_out.groupby('nameOrig')['amount'].sum().to_dict()\n",
        "\n",
        "all_user_ids = set(total_received.keys()) | set(total_cashed_out.keys()) | set(unique_senders.keys())\n",
        "profiles_list = []\n",
        "for user_id in all_user_ids:\n",
        "    received = total_received.get(user_id, 0)\n",
        "    cashed_out = total_cashed_out.get(user_id, 0)\n",
        "    senders = unique_senders.get(user_id, 0)\n",
        "    ratio = (cashed_out / (received + 1e-6))\n",
        "    ratio = min(ratio, 1.0)\n",
        "    profiles_list.append({\n",
        "        'user_id': user_id,\n",
        "        'dest_cash_out_ratio': ratio,\n",
        "        'dest_unique_senders': senders\n",
        "    })\n",
        "final_profiles = pd.DataFrame(profiles_list)\n",
        "print(\"Behavioral profiles created successfully.\")\n",
        "\n",
        "# --- Step 3: Merge Features with Transactions ---\n",
        "print(\"\\n--- Step 3: Merging Features with Transactions ---\")\n",
        "df_model_data = pd.merge(df_smart_sample, final_profiles, left_on='nameDest', right_on='user_id', how='left')\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameOrig', right_on='user_id', how='left', suffixes=('_dest', '_orig'))\n",
        "\n",
        "df_model_data['dest_cash_out_ratio_dest'] = df_model_data['dest_cash_out_ratio_dest'].fillna(0)\n",
        "df_model_data['dest_unique_senders_dest'] = df_model_data['dest_unique_senders_dest'].fillna(0)\n",
        "df_model_data['dest_cash_out_ratio_orig'] = df_model_data['dest_cash_out_ratio_orig'].fillna(0)\n",
        "df_model_data['dest_unique_senders_orig'] = df_model_data['dest_unique_senders_orig'].fillna(0)\n",
        "\n",
        "# --- Step 4: Train Unsupervised LOF Model ---\n",
        "print(\"\\n--- Step 4: Training Local Outlier Factor (LOF) Model ---\")\n",
        "\n",
        "features = [\n",
        "    'amount',\n",
        "    'dest_cash_out_ratio_dest',\n",
        "    'dest_unique_senders_dest',\n",
        "    'dest_cash_out_ratio_orig',\n",
        "    'dest_unique_senders_orig'\n",
        "]\n",
        "df_model_data['type_encoded'] = df_model_data['type'].astype('category').cat.codes\n",
        "features.append('type_encoded')\n",
        "\n",
        "X = df_model_data[features]\n",
        "y_true = df_model_data['isFraud'] # The \"Correct Answer\" (for comparison only)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "contamination = y_true.mean()\n",
        "print(f\"Fraud (Contamination) rate in the Smart Sample: {contamination:.2%}\")\n",
        "\n",
        "# --- (This is the change) ---\n",
        "# n_neighbors=20 is a common default setting\n",
        "model = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n",
        "\n",
        "# LOF uses .fit_predict() to give us -1 (anomalous) or 1 (normal)\n",
        "predictions = model.fit_predict(X_scaled)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 5: Evaluation (Comparison) ---\n",
        "print(\"\\n--- Step 5: Evaluating the LOF Model ---\")\n",
        "\n",
        "# Convert the model's \"guess\" (-1) to (1) to match 'isFraud'\n",
        "y_pred = [1 if p == -1 else 0 for p in predictions]\n",
        "\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.2%}\")\n",
        "print(f\"Recall: {recall:.2%}\")\n",
        "print(f\"F1-Score (Final score for LOF): {f1:.2%}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"\\n--- LOF Code Execution Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGqIZs7_9DYx",
        "outputId": "e6cbc87e-3e26-46eb-dffd-6e9b99a9c71e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Execution Started (Comparison with LOF) ---\n",
            "Objective: Test LOF's performance on the same behavioral features.\n",
            "Successfully loaded the full file (490697 rows).\n",
            "\n",
            "--- Step 1: Creating the 'Smart Sample' ---\n",
            "The final 'Smart Sample' was created with 490696 rows.\n",
            "\n",
            "--- Step 2: Building Behavioral Profiles ---\n",
            "Behavioral profiles created successfully.\n",
            "\n",
            "--- Step 3: Merging Features with Transactions ---\n",
            "\n",
            "--- Step 4: Training Local Outlier Factor (LOF) Model ---\n",
            "Fraud (Contamination) rate in the Smart Sample: 0.05%\n",
            "Model training complete.\n",
            "\n",
            "--- Step 5: Evaluating the LOF Model ---\n",
            "Precision: 9.52%\n",
            "Recall: 9.52%\n",
            "F1-Score (Final score for LOF): 9.52%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[490256    209]\n",
            " [   209     22]]\n",
            "\n",
            "--- LOF Code Execution Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "# --- (Change: We will use neural network libraries) ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# Ignore unimportant warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"--- Execution Started (Smarter Autoencoder Model) ---\")\n",
        "print(\"Objective: Use a neural network (Autoencoder) to learn 'normal behavior' only.\")\n",
        "\n",
        "# --- Load Data ---\n",
        "file_path = 'PS_20174392719_1491204439457_log.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Successfully loaded the full file ({len(df)} rows).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 1: Building Behavioral Profiles (Full Dataset) ---\n",
        "# (We need to build profiles on the full dataset for accuracy)\n",
        "print(\"\\n--- Step 1: Building Behavioral Profiles (Full) ---\")\n",
        "df_received = df[df['type'].isin(['TRANSFER', 'CASH_IN'])]\n",
        "total_received = df_received.groupby('nameDest')['amount'].sum().to_dict()\n",
        "unique_senders = df_received.groupby('nameDest')['nameOrig'].nunique().to_dict()\n",
        "df_cashed_out = df[df['type'] == 'CASH_OUT']\n",
        "total_cashed_out = df_cashed_out.groupby('nameOrig')['amount'].sum().to_dict()\n",
        "\n",
        "all_user_ids = set(total_received.keys()) | set(total_cashed_out.keys()) | set(unique_senders.keys())\n",
        "profiles_list = []\n",
        "for user_id in all_user_ids:\n",
        "    received = total_received.get(user_id, 0)\n",
        "    cashed_out = total_cashed_out.get(user_id, 0)\n",
        "    senders = unique_senders.get(user_id, 0)\n",
        "    ratio = (cashed_out / (received + 1e-6))\n",
        "    ratio = min(ratio, 1.0)\n",
        "    profiles_list.append({\n",
        "        'user_id': user_id,\n",
        "        'dest_cash_out_ratio': ratio,\n",
        "        'dest_unique_senders': senders\n",
        "    })\n",
        "final_profiles = pd.DataFrame(profiles_list)\n",
        "print(\"Behavioral profiles created successfully.\")\n",
        "\n",
        "# --- Step 2: Merge Features with Transactions ---\n",
        "print(\"\\n--- Step 2: Merging Features with Transactions ---\")\n",
        "df_model_data = pd.merge(df, final_profiles, left_on='nameDest', right_on='user_id', how='left')\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameOrig', right_on='user_id', how='left', suffixes=('_dest', '_orig'))\n",
        "\n",
        "# Fill NaNs from the merge\n",
        "df_model_data['dest_cash_out_ratio_dest'] = df_model_data['dest_cash_out_ratio_dest'].fillna(0)\n",
        "df_model_data['dest_unique_senders_dest'] = df_model_data['dest_unique_senders_dest'].fillna(0)\n",
        "df_model_data['dest_cash_out_ratio_orig'] = df_model_data['dest_cash_out_ratio_orig'].fillna(0)\n",
        "df_model_data['dest_unique_senders_orig'] = df_model_data['dest_unique_senders_orig'].fillna(0)\n",
        "\n",
        "# --- Step 3: Prepare Train/Test Data ---\n",
        "print(\"\\n--- Step 3: Preparing Train/Test Data ---\")\n",
        "\n",
        "# --- (FIX: Drop rows where the label 'isFraud' is NaN) ---\n",
        "# The stratify parameter in train_test_split cannot handle NaN labels.\n",
        "# These rows are unusable for training or testing anyway.\n",
        "print(f\"Original data size: {len(df_model_data)}\")\n",
        "df_model_data = df_model_data.dropna(subset=['isFraud'])\n",
        "print(f\"Data size after dropping NaN labels: {len(df_model_data)}\")\n",
        "# --- (End of Fix) ---\n",
        "\n",
        "features_list = [\n",
        "    'amount',\n",
        "    'dest_cash_out_ratio_dest',\n",
        "    'dest_unique_senders_dest',\n",
        "    'dest_cash_out_ratio_orig',\n",
        "    'dest_unique_senders_orig'\n",
        "]\n",
        "df_model_data['type_encoded'] = df_model_data['type'].astype('category').cat.codes\n",
        "features_list.append('type_encoded')\n",
        "\n",
        "X_all_features = df_model_data[features_list]\n",
        "y_all_labels = df_model_data['isFraud'] # The \"Correct Answer\"\n",
        "\n",
        "# (1) Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_all_features_scaled = scaler.fit_transform(X_all_features)\n",
        "\n",
        "# (2) Split the data (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_all_features_scaled, y_all_labels,\n",
        "    test_size=0.3, random_state=42, stratify=y_all_labels\n",
        ")\n",
        "\n",
        "# (3) (Most important step) Create 'clean training data'\n",
        "# The model will train *only* on the normal transactions from the training set\n",
        "X_train_normal = X_train[y_train == 0]\n",
        "print(f\"Data split. We will train on {len(X_train_normal)} 'normal' transactions.\")\n",
        "\n",
        "# --- Step 4: Build the Autoencoder Model ---\n",
        "print(\"\\n--- Step 4: Building the Autoencoder Model ---\")\n",
        "input_dim = X_train_normal.shape[1] # Number of features\n",
        "\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "\n",
        "# Encoder (The \"compression\" part)\n",
        "encoder = Dense(input_dim // 2, activation='relu')(input_layer) # 6 -> 3\n",
        "encoder = Dense(input_dim // 4, activation='relu')(encoder)     # 3 -> 1 (The \"bottleneck\")\n",
        "\n",
        "# Decoder (The \"reconstruction\" part)\n",
        "decoder = Dense(input_dim // 2, activation='relu')(encoder)     # 1 -> 3\n",
        "decoder = Dense(input_dim, activation='linear')(decoder)        # 3 -> 6 (Original shape)\n",
        "\n",
        "# Assemble the model\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "print(\"Model built successfully.\")\n",
        "autoencoder.summary()\n",
        "\n",
        "# --- Step 5: Train the Model (On Normal Data Only) ---\n",
        "print(\"\\n--- Step 5: Training the model (on normal data only)... ---\")\n",
        "# The model \"learns\" how to reconstruct normal training data\n",
        "autoencoder.fit(\n",
        "    X_train_normal, X_train_normal,\n",
        "    epochs=10, # 10 training cycles (can be increased)\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    validation_data=(X_test, X_test), # It validates on the full test set\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 6: Evaluate the Model (Calculate \"Reconstruction Error\") ---\n",
        "print(\"\\n--- Step 6: Evaluating the Model ---\")\n",
        "\n",
        "# (a) Calculate the reconstruction error for every transaction in the test set\n",
        "predictions = autoencoder.predict(X_test)\n",
        "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
        "df_test = pd.DataFrame({'Reconstruction_Error': mse, 'True_Label': y_test})\n",
        "\n",
        "# (b) Determine the anomaly \"Threshold\"\n",
        "# We will flag anything \"weirder\" than 99% of the normal transactions\n",
        "# (We use the clean training data to set this threshold)\n",
        "train_predictions = autoencoder.predict(X_train_normal)\n",
        "train_mse = np.mean(np.power(X_train_normal - train_predictions, 2), axis=1)\n",
        "threshold = np.quantile(train_mse, 0.99) # Set threshold at the 99th percentile\n",
        "print(f\"Anomaly threshold determined at: {threshold:.4f}\")\n",
        "\n",
        "# (c) Make Predictions\n",
        "# Any transaction whose \"error\" is higher than the threshold = anomaly (1)\n",
        "y_pred = [1 if e > threshold else 0 for e in df_test['Reconstruction_Error']]\n",
        "\n",
        "# (d) Calculate the final score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\n--- (Final Result for Autoencoder Model) ---\")\n",
        "print(f\"Precision: {precision:.2%}\")\n",
        "print(f\"Recall: {recall:.2%}\")\n",
        "print(f\"F1-Score (New Final Score): {f1:.2%}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-aGqNZb-AAS1",
        "outputId": "e1c44c79-7f23-4152-d597-3419de3a8b65"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Execution Started (Smarter Autoencoder Model) ---\n",
            "Objective: Use a neural network (Autoencoder) to learn 'normal behavior' only.\n",
            "Successfully loaded the full file (1159457 rows).\n",
            "\n",
            "--- Step 1: Building Behavioral Profiles (Full) ---\n",
            "Behavioral profiles created successfully.\n",
            "\n",
            "--- Step 2: Merging Features with Transactions ---\n",
            "\n",
            "--- Step 3: Preparing Train/Test Data ---\n",
            "Original data size: 1159457\n",
            "Data size after dropping NaN labels: 1159456\n",
            "Data split. We will train on 810563 'normal' transactions.\n",
            "\n",
            "--- Step 4: Building the Autoencoder Model ---\n",
            "Model built successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                           \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                          \u001b[38;5;34m21\u001b[0m \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                           \u001b[38;5;34m4\u001b[0m \n",
              "\n",
              " dense_2 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m6\u001b[0m \n",
              "\n",
              " dense_3 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                          \u001b[38;5;34m24\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> \n",
              "\n",
              " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> \n",
              "\n",
              " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55\u001b[0m (220.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> (220.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55\u001b[0m (220.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> (220.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 5: Training the model (on normal data only)... ---\n",
            "Epoch 1/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2ms/step - loss: 0.7424 - val_loss: 0.5533\n",
            "Epoch 2/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2ms/step - loss: 0.6964 - val_loss: 0.4487\n",
            "Epoch 3/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2ms/step - loss: 0.5880 - val_loss: 0.4374\n",
            "Epoch 4/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2ms/step - loss: 0.6366 - val_loss: 0.4303\n",
            "Epoch 5/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2ms/step - loss: 0.5308 - val_loss: 0.4317\n",
            "Epoch 6/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2ms/step - loss: 0.4675 - val_loss: 0.4288\n",
            "Epoch 7/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2ms/step - loss: 0.5977 - val_loss: 0.4300\n",
            "Epoch 8/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3ms/step - loss: 0.4973 - val_loss: 0.4327\n",
            "Epoch 9/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2ms/step - loss: 0.5704 - val_loss: 0.4291\n",
            "Epoch 10/10\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2ms/step - loss: 0.5759 - val_loss: 0.4293\n",
            "Model training complete.\n",
            "\n",
            "--- Step 6: Evaluating the Model ---\n",
            "\u001b[1m10870/10870\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step\n",
            "\u001b[1m25331/25331\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step\n",
            "Anomaly threshold determined at: 4.4695\n",
            "\n",
            "--- (Final Result for Autoencoder Model) ---\n",
            "Precision: 2.63%\n",
            "Recall: 21.68%\n",
            "F1-Score (New Final Score): 4.70%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[343762   3623]\n",
            " [   354     98]]\n"
          ]
        }
      ]
    }
  ]
}